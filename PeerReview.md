**Shubham's Review**

Roles were created through the "create role" command and organized into a hierarchy based on specific requirements. Following this, a data warehouse was established by utilizing the account admin role. After switching to the admin role, privileges were granted, and a database was created.Subsequently, a schema was created based on the requirements outlined in the question. An employee data table was also created in accordance with the schema of a CSV file. Integration with AWS, file formats, and an external staging environment were set up to facilitate data loading.A variant was created, and an internal stage was created to load data into the variant table using the "insert into" command. The CSV file was then loaded into the created table from a local source by running the "put" command in the command prompt. By utilizing the "copy" command, data was loaded into both internal and external tables.Queries were then executed on the above tables, and a parquet file format was created using the "put" command. Data was subsequently loaded into this format, and the "infer_schema" query command was run on the parquet stage. Masking policies were created and applied to columns in tables.
Finally, different privileges were granted to the pii and developer roles, and query commands were executed on tables from different roles.


**Kushagra's Review**

Firstly, using the "create role" command, roles were established and organized into a hierarchy according to specific requirements. Next, a data warehouse was created from the account admin role. After switching to the admin role, privileges were granted and a database was created.Following this, a schema was designed in accordance with the given requirements, and an employee data table was created based on the schema of a CSV file. In addition, a variant table and file format were created. By using the "put" command, a file was loaded into the internal stage from a local source. Data was then loaded into a table using the "copy" command.A file format was also created for the external stage, and AWS was integrated with Snowflake. Data was subsequently loaded into the external stage from an AWS S3 bucket, and the "put" command was used to load data into the internal stage. Data was then inserted into both the external and internal tables.Moreover, a file format was created for the parquet file, and data was loaded into the stage using the "put" command. The "infer schema" was then extracted using a select query, and query commands were executed on the parquet stage.Additionally, masking policies were created and applied to tables, and different privileges were granted to the pii and developer roles. Finally, queries were executed on tables from different roles.
